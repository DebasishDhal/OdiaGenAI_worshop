{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning falcon-1b with Axolotl+QLoRA\n",
        "\n",
        "This notebook makes it easy to try out finetuning falcon-1b with Axolotl+QLoRA.\n",
        "\n",
        "axolotl link [here](https://github.com/OpenAccess-AI-Collective/axolotl) ."
      ],
      "metadata": {
        "id": "fNGIsnKzvJ_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Axolotl and install its dependencies"
      ],
      "metadata": {
        "id": "rBXtcMZ1v2Ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv_I9Gl2l5Co",
        "outputId": "783b4eb4-21cc-4371-b68c-ef063d97aa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 7534, done.\u001b[K\n",
            "remote: Counting objects: 100% (1735/1735), done.\u001b[K\n",
            "remote: Compressing objects: 100% (394/394), done.\u001b[K\n",
            "remote: Total 7534 (delta 1532), reused 1417 (delta 1318), pack-reused 5799\u001b[K\n",
            "Receiving objects: 100% (7534/7534), 2.64 MiB | 6.62 MiB/s, done.\n",
            "Resolving deltas: 100% (4861/4861), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd axolotl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2zoaq0soj7_",
        "outputId": "9119f992-4440-41a4-a0ad-fb323f932a93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install -e ."
      ],
      "metadata": {
        "id": "ktKRVi2woiEK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate default config for accelerate\n",
        "\n"
      ],
      "metadata": {
        "id": "baxv4s9DwJyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEA8tDYtnMiz",
        "outputId": "f876a484-960d-492a-a650-71360b7a0207"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.cache/huggingface/accelerate/default_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzOrEqiKopPG",
        "outputId": "5c87fe1e-746f-42fe-8585-ac03732b5a54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
            "  \"debug\": false,\n",
            "  \"distributed_type\": \"NO\",\n",
            "  \"downcast_bf16\": false,\n",
            "  \"machine_rank\": 0,\n",
            "  \"main_training_function\": \"main\",\n",
            "  \"mixed_precision\": \"no\",\n",
            "  \"num_machines\": 1,\n",
            "  \"num_processes\": 1,\n",
            "  \"rdzv_backend\": \"static\",\n",
            "  \"same_network\": false,\n",
            "  \"tpu_use_cluster\": false,\n",
            "  \"tpu_use_sudo\": false,\n",
            "  \"use_cpu\": false\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Use a well-tested falcon-7b qlora config and adjust it to 1b & Cloab"
      ],
      "metadata": {
        "id": "XlAWF78hwP15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sam-ai/axolotl/main/examples/falcon/config-1b-qlora.yml -O /content/axolotl/examples/falcon/config-1b-qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9iKr3wydAmD",
        "outputId": "aa44d865-4fc8-41b1-9251-8f6c263b93b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 04:34:56--  https://raw.githubusercontent.com/sam-ai/axolotl/main/examples/falcon/config-1b-qlora.yml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2228 (2.2K) [text/plain]\n",
            "Saving to: ‘/content/axolotl/examples/falcon/config-1b-qlora.yml’\n",
            "\n",
            "/content/axolotl/ex 100%[===================>]   2.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-23 04:34:56 (34.3 MB/s) - ‘/content/axolotl/examples/falcon/config-1b-qlora.yml’ saved [2228/2228]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add hub_model_id: <hugginface-repo-path> in config file"
      ],
      "metadata": {
        "id": "M0rRHdTud0YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/axolotl/examples/falcon/config-1b-qlora.yml"
      ],
      "metadata": {
        "id": "K8lhS_nepOLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0694eaff-d061-4127-8ec0-809a84b06384"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model: tiiuae/falcon-rw-1b\n",
            "base_model_config: tiiuae/falcon-rw-1b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "is_falcon_derived_model: true\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "push_dataset_to_hub:\n",
            "datasets:\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca:instruct\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "# lora_modules_to_save:\n",
            "#   - word_embeddings\n",
            "#   - lm_head\n",
            "\n",
            "wandb_project: falcon-1B\n",
            "wandb_entity:\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: ./qlora-out\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 6\n",
            "gradient_accumulation_steps: 2\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: false\n",
            "fp16: false\n",
            "tf32: false\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 50\n",
            "save_steps: 100\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.000001\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zy1OPV19p0p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read the comments in the config and tweak it."
      ],
      "metadata": {
        "id": "dyfdi-4FykZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Install QLoRA dependencies"
      ],
      "metadata": {
        "id": "vdfnM4pWwilY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "id": "IZN2Za6BrL2S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Set W&B to offline mode"
      ],
      "metadata": {
        "id": "XF1CKnYGw8gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_MODE=online"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zOFnyyrunwG",
        "outputId": "9c8ec61e-ec3d-49a9-d9c6-5431397cd7cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_MODE=offline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is to skip some extra setup steps, you can also choose to login to your W&B account  before training.\n"
      ],
      "metadata": {
        "id": "5KgCQQ7yxY5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Start training and enjoy!"
      ],
      "metadata": {
        "id": "Ocr3ddm3yXwU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdxopwRncQDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train /content/axolotl/examples/falcon/config-1b-qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhsJlzRTqE7n",
        "outputId": "91a7f9e6-dbc6-4488-a274-0ed77ea36ae8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-23 04:40:38.597349: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-23 04:40:38.597423: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-23 04:40:38.597461: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-23 04:40:40.681662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "\u001b[33m[2023-11-23 04:40:45,857] [WARNING] [axolotl.validate_config:169] [PID:4082] [RANK:0] eval_batch_size != micro_batch_size. This can lead to VRAM instability.\u001b[39m\n",
            "\u001b[33m[2023-11-23 04:40:45,857] [WARNING] [axolotl.validate_config:224] [PID:4082] [RANK:0] `trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\u001b[39m\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 5.92MB/s]\n",
            "configuration_falcon.py: 100% 6.70k/6.70k [00:00<00:00, 31.7MB/s]\n",
            "[2023-11-23 04:40:46,404] [INFO] [axolotl.normalize_config:128] [PID:4082] [RANK:0] GPU memory usage baseline: 0.000GB (+0.255GB misc)\u001b[39m\n",
            "\u001b[33m[2023-11-23 04:40:46,404] [WARNING] [axolotl.scripts.check_accelerate_default_config:343] [PID:4082] [RANK:0] accelerate config file found at /root/.cache/huggingface/accelerate/default_config.yaml. This can lead to unexpected errors\u001b[39m\n",
            "\u001b[33m[2023-11-23 04:40:46,405] [WARNING] [axolotl.scripts.check_user_token:355] [PID:4082] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.\u001b[39m\n",
            "tokenizer_config.json: 100% 234/234 [00:00<00:00, 1.64MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 3.50MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 7.88MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 9.49MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 730kB/s]\n",
            "[2023-11-23 04:40:48,077] [DEBUG] [axolotl.load_tokenizer:100] [PID:4082] [RANK:0] EOS: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:40:48,077] [DEBUG] [axolotl.load_tokenizer:101] [PID:4082] [RANK:0] BOS: 50257 / >>ABSTRACT<<\u001b[39m\n",
            "[2023-11-23 04:40:48,077] [DEBUG] [axolotl.load_tokenizer:102] [PID:4082] [RANK:0] PAD: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:40:48,077] [DEBUG] [axolotl.load_tokenizer:103] [PID:4082] [RANK:0] UNK: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:40:48,078] [INFO] [axolotl.load_tokenized_prepared_datasets:147] [PID:4082] [RANK:0] Unable to find prepared dataset in last_run_prepared/5c60f9f8e73ae88b5cfc88f2775c049f\u001b[39m\n",
            "[2023-11-23 04:40:48,078] [INFO] [axolotl.load_tokenized_prepared_datasets:148] [PID:4082] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "[2023-11-23 04:40:48,078] [INFO] [axolotl.load_tokenized_prepared_datasets:153] [PID:4082] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "Downloading readme: 100% 501/501 [00:00<00:00, 4.19MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/36.0M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  12% 4.19M/36.0M [00:00<00:02, 12.3MB/s]\u001b[A\n",
            "Downloading data:  35% 12.6M/36.0M [00:00<00:00, 24.2MB/s]\u001b[A\n",
            "Downloading data:  58% 21.0M/36.0M [00:00<00:00, 26.0MB/s]\u001b[A\n",
            "Downloading data:  82% 29.4M/36.0M [00:01<00:00, 30.4MB/s]\u001b[A\n",
            "Downloading data: 100% 36.0M/36.0M [00:01<00:00, 28.8MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/4.91M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 4.91M/4.91M [00:00<00:00, 13.6MB/s]\n",
            "Downloading data files: 100% 1/1 [00:01<00:00,  1.62s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 502.37it/s]\n",
            "Generating train split: 54568 examples [00:00, 114712.36 examples/s]\n",
            "Map (num_proc=2):  67% 36826/54568 [00:38<00:12, 1425.22 examples/s]\u001b[33m[2023-11-23 04:41:32,344] [WARNING] [axolotl._tokenize:66] [PID:4196] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Map (num_proc=2): 100% 54568/54568 [00:55<00:00, 978.07 examples/s]\n",
            "[2023-11-23 04:41:49,788] [INFO] [axolotl.load_tokenized_prepared_datasets:362] [PID:4082] [RANK:0] merging datasets\u001b[39m\n",
            "[2023-11-23 04:41:49,791] [INFO] [axolotl.load_tokenized_prepared_datasets:369] [PID:4082] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/5c60f9f8e73ae88b5cfc88f2775c049f\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 54568/54568 [00:00<00:00, 239388.98 examples/s]\n",
            "Filter (num_proc=2): 100% 54022/54022 [00:13<00:00, 4008.70 examples/s]\n",
            "Filter (num_proc=2): 100% 546/546 [00:00<00:00, 1348.70 examples/s]\n",
            "[2023-11-23 04:42:04,291] [DEBUG] [axolotl.log:60] [PID:4082] [RANK:0] total_num_tokens: 10335073\u001b[39m\n",
            "[2023-11-23 04:42:05,120] [DEBUG] [axolotl.log:60] [PID:4082] [RANK:0] `total_supervised_tokens: 6525025`\u001b[39m\n",
            "[2023-11-23 04:42:05,120] [DEBUG] [axolotl.log:60] [PID:4082] [RANK:0] total_num_steps: 13505\u001b[39m\n",
            "[2023-11-23 04:42:05,121] [DEBUG] [axolotl.train.log:60] [PID:4082] [RANK:0] loading tokenizer... tiiuae/falcon-rw-1b\u001b[39m\n",
            "[2023-11-23 04:42:05,308] [DEBUG] [axolotl.load_tokenizer:100] [PID:4082] [RANK:0] EOS: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:42:05,308] [DEBUG] [axolotl.load_tokenizer:101] [PID:4082] [RANK:0] BOS: 50257 / >>ABSTRACT<<\u001b[39m\n",
            "[2023-11-23 04:42:05,308] [DEBUG] [axolotl.load_tokenizer:102] [PID:4082] [RANK:0] PAD: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:42:05,308] [DEBUG] [axolotl.load_tokenizer:103] [PID:4082] [RANK:0] UNK: 50256 / <|endoftext|>\u001b[39m\n",
            "[2023-11-23 04:42:05,309] [DEBUG] [axolotl.train.log:60] [PID:4082] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "modeling_falcon.py: 100% 56.9k/56.9k [00:00<00:00, 1.02MB/s]\n",
            "pytorch_model.bin: 100% 2.62G/2.62G [00:18<00:00, 141MB/s]\n",
            "generation_config.json: 100% 115/115 [00:00<00:00, 564kB/s]\n",
            "[2023-11-23 04:42:42,403] [INFO] [axolotl.load_model:389] [PID:4082] [RANK:0] GPU memory usage after model load: 0.968GB (+0.015GB cache, +0.921GB misc)\u001b[39m\n",
            "[2023-11-23 04:42:42,408] [INFO] [axolotl.load_model:406] [PID:4082] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2023-11-23 04:42:42,415] [INFO] [axolotl.load_model:417] [PID:4082] [RANK:0] converting modules to torch.float32 for flash attention\u001b[39m\n",
            "[2023-11-23 04:42:42,418] [INFO] [axolotl.load_lora:519] [PID:4082] [RANK:0] found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']\u001b[39m\n",
            "trainable params: 50,331,648 || all params: 1,361,956,864 || trainable%: 3.6955390681154494\n",
            "[2023-11-23 04:42:43,434] [INFO] [axolotl.load_model:446] [PID:4082] [RANK:0] GPU memory usage after adapters: 1.157GB (+0.019GB cache, +0.921GB misc)\u001b[39m\n",
            "[2023-11-23 04:42:43,448] [INFO] [axolotl.train.log:60] [PID:4082] [RANK:0] Pre-saving adapter config to ./qlora-out\u001b[39m\n",
            "[2023-11-23 04:42:43,525] [INFO] [axolotl.train.log:60] [PID:4082] [RANK:0] Starting trainer...\u001b[39m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "[2023-11-23 04:42:46,003] [INFO] [axolotl.callbacks.on_train_begin:537] [PID:4082] [RANK:0] Axolotl config has been saved to WandB as an artifact.\u001b[39m\n",
            "{'loss': 1.4372, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
            "  0% 1/13506 [00:11<42:40:29, 11.38s/it][2023-11-23 04:43:03,084] [INFO] [axolotl.callbacks.on_step_end:122] [PID:4082] [RANK:0] GPU memory usage while training: 1.173GB (+2.979GB cache, +1.517GB misc)\u001b[39m\n",
            "{'loss': 1.8166, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
            "{'loss': 1.8184, 'learning_rate': 6e-05, 'epoch': 0.0}\n",
            "{'loss': 1.5289, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
            "{'loss': 1.6651, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
            "{'loss': 1.6841, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
            "{'loss': 1.4294, 'learning_rate': 0.00014, 'epoch': 0.0}\n",
            "{'loss': 1.564, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
            "{'loss': 1.3562, 'learning_rate': 0.00018, 'epoch': 0.0}\n",
            "{'loss': 1.3641, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
            "{'loss': 1.3518, 'learning_rate': 0.00019999999729068435, 'epoch': 0.0}\n",
            "{'loss': 1.4602, 'learning_rate': 0.00019999998916273747, 'epoch': 0.0}\n",
            "{'loss': 1.4481, 'learning_rate': 0.00019999997561615987, 'epoch': 0.0}\n",
            "{'loss': 1.3939, 'learning_rate': 0.00019999995665095224, 'epoch': 0.0}\n",
            "{'loss': 1.3981, 'learning_rate': 0.00019999993226711563, 'epoch': 0.0}\n",
            "{'loss': 1.6473, 'learning_rate': 0.00019999990246465137, 'epoch': 0.0}\n",
            "{'loss': 1.6384, 'learning_rate': 0.00019999986724356103, 'epoch': 0.0}\n",
            "{'loss': 1.4076, 'learning_rate': 0.00019999982660384656, 'epoch': 0.0}\n",
            "{'loss': 1.5791, 'learning_rate': 0.00019999978054551014, 'epoch': 0.0}\n",
            "{'loss': 1.2685, 'learning_rate': 0.00019999972906855427, 'epoch': 0.0}\n",
            "{'loss': 1.3067, 'learning_rate': 0.00019999967217298177, 'epoch': 0.0}\n",
            "{'loss': 1.3887, 'learning_rate': 0.0001999996098587957, 'epoch': 0.0}\n",
            "{'loss': 1.2872, 'learning_rate': 0.0001999995421259994, 'epoch': 0.01}\n",
            "{'loss': 1.4973, 'learning_rate': 0.0001999994689745966, 'epoch': 0.01}\n",
            "{'loss': 1.1632, 'learning_rate': 0.0001999993904045912, 'epoch': 0.01}\n",
            "{'loss': 1.2634, 'learning_rate': 0.00019999930641598752, 'epoch': 0.01}\n",
            "{'loss': 1.5333, 'learning_rate': 0.0001999992170087901, 'epoch': 0.01}\n",
            "{'loss': 1.4004, 'learning_rate': 0.00019999912218300375, 'epoch': 0.01}\n",
            "{'loss': 1.7364, 'learning_rate': 0.00019999902193863363, 'epoch': 0.01}\n",
            "{'loss': 1.5828, 'learning_rate': 0.00019999891627568519, 'epoch': 0.01}\n",
            "  0% 30/13506 [03:30<22:48:46,  6.09s/it]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 994, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 633, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "  0% 30/13506 [03:32<26:32:42,  7.09s/it]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ▁▂▃▃▄▅▆▆▇█████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▄██▅▆▇▄▅▃▃▃▄▄▃▄▆▆▄▅▂▃▃▂▅▁▂▅▄▇▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 0.0002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.5828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/axolotl/wandb/offline-run-20231123_044245-b49bhnkw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20231123_044245-b49bhnkw/logs\u001b[0m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/drive/1AfE71MfayK4qo9DxdTR5oAeJdtZtVtyG#offline=true&sandboxMode=true"
      ],
      "metadata": {
        "id": "cNKVb2ERrgRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}